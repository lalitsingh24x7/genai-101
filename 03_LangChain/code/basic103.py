"""
basic103.py

This script demonstrates how to use LangChain's PromptTemplate and modern chaining syntax
to generate a question about a topic and then answer it using the OpenAI GPT-4o-mini model.

Dependencies:
    - python-dotenv
    - langchain_openai

Usage:
    Ensure a valid OpenAI API key is set in the .env file as OPENAI_API_KEY.
    Run the script to see the model generate and answer a question about the given topic.
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate

# Load environment variables from .env file (for API keys, etc.)
load_dotenv()

# Initialize the OpenAI chat model with specified parameters
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

# Define a prompt template to generate a question about a given topic
question_prompt = PromptTemplate(
    input_variables=["topic"],
    template="Generate a simple question aobut {topic}"
)

# Define a prompt template to answer a given question concisely
answer_prompt = PromptTemplate(
    input_variables=["question"],
    template="Answer the following question concisly : {question}"
)

# Create a chain: 
# 1. Extract 'topic' from input
# 2. Generate a question about the topic
# 3. Use LLM to get the question
# 4. Extract the question from LLM output
# 5. Generate an answer prompt for the question
# 6. Use LLM to get the answer
chain = (
    {"topic": lambda x: x["topic"]}
    | question_prompt
    | llm
    | (lambda x: {"question": x.content })
    | answer_prompt
    | llm
)

# Set the topic for the question generation
topic = "about indian cricket team"

# Invoke the chain with the topic and get the final answer
result = chain.invoke({"topic": topic})

# Print the answer generated by the chain
print(result.content)